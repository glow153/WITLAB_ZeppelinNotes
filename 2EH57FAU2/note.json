{
  "paragraphs": [
    {
      "text": "/**\n * This anomaly detection code mainly follows the tutorial from Sean Owen, Cloudera.\n * Couple of modifcations have been made to fit personal interest:\n * \t\t- Instead of training multiple clusters, the code only trains on \"normal\" data points\n * \t\t- Only one cluster center is recorded and threshold is set to the last of \n *\t\t  the furthest 3000 data points\n *\t\t- During later validating stage, all points that are further than the threshold\n *\t\t  is labeled as \"anomaly\"\n *\n * Video: https://www.youtube.com/watch?v\u003dTC5cKYBZAeI\n * Slides-1: http://www.slideshare.net/CIGTR/anomaly-detection-with-apache-spark\n * Slides-2: http://www.slideshare.net/CIGTR/anomaly-detection-with-apache-spark-2\n */ ",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.322",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1562221936321_-805270085",
      "id": "20151224-130250_1885393244",
      "dateCreated": "2019-07-04 15:32:16.321",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//************************************\n//         Training\n//************************************\n\n// load the data\nval rawData \u003d sc.textFile(\"dataset/ad.train.csv\", 120)\nrawData.count",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.323",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rawData: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[3] at textFile at \u003cconsole\u003e:24\nres2: Long \u003d 766639\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936323_883885413",
      "id": "20151224-130024_2027641500",
      "dateCreated": "2019-07-04 15:32:16.323",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.324",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936324_1196281372",
      "id": "20151224-125855_148192554",
      "dateCreated": "2019-07-04 15:32:16.324",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// parse the data\nval dataAndLabel \u003d rawData.map { line \u003d\u003e\n\tval buffer \u003d ArrayBuffer[String]()\n\tbuffer.appendAll(line.split(\",\"))\n\tbuffer.remove(1, 3) // remove categorial attributes\n\tval label \u003d buffer.remove(buffer.length-1)\n\tval vector \u003d Vectors.dense(buffer.map(_.toDouble).toArray) \n\t(vector, label)\n}",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.325",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "dataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] \u003d MapPartitionsRDD[4] at map at \u003cconsole\u003e:29\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936325_1028373753",
      "id": "20151224-125934_1313969834",
      "dateCreated": "2019-07-04 15:32:16.325",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// checkout how data look like\ndataAndLabel.take(1)\ndataAndLabel.map(_._1).take(1)\ndataAndLabel.map(_._2).take(1)",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.326",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res7: Array[(org.apache.spark.mllib.linalg.Vector, String)] \u003d Array(([0.0,215.0,45076.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],normal.))\nres8: Array[org.apache.spark.mllib.linalg.Vector] \u003d Array([0.0,215.0,45076.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\nres9: Array[String] \u003d Array(normal.)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936325_-284998014",
      "id": "20151224-130209_551414281",
      "dateCreated": "2019-07-04 15:32:16.325",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// training data\nval data \u003d dataAndLabel.map(_._1).cache()",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.327",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] \u003d MapPartitionsRDD[7] at map at \u003cconsole\u003e:32\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936326_-156026791",
      "id": "20151224-130745_211188629",
      "dateCreated": "2019-07-04 15:32:16.326",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// normalize training data\nval dataArray \u003d data.map(_.toArray)\nval numCols \u003d dataArray.first().length\nval n \u003d dataArray.count()\n\nval sums \u003d dataArray.reduce((a, b) \u003d\u003e a.zip(b).map(t \u003d\u003e t._1 + t._2))\nval sumSquares \u003d dataArray.fold(new Array[Double](numCols)) (\n\t(a,b) \u003d\u003e a.zip(b).map(t \u003d\u003e t._1 + t._2 * t._2)\n\t)\nval stdevs \u003d sumSquares.zip(sums).map { case\n\t(sumSq, sum) \u003d\u003e math.sqrt(n * sumSq - sum * sum) / n\n}\nval means \u003d sums.map(_ / n)\n\n// normaization function\ndef normalize(v: Vector) \u003d {\n\tval normed \u003d (v.toArray, means, stdevs).zipped.map { \n\t\tcase (value, mean, 0) \u003d\u003e (value - mean) / 1 // if stdev is 0\n\t\tcase (value, mean, stdev) \u003d\u003e (value - mean) / stdev\n\t\t}\n\tVectors.dense(normed)\n}\n\n// apply the function\nval normalizedData \u003d data.map(normalize(_))\n\n// put the label back\nval normalizedDataAndLabel \u003d normalizedData.zip(dataAndLabel.values)\n",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.327",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "dataArray: org.apache.spark.rdd.RDD[Array[Double]] \u003d MapPartitionsRDD[111] at map at \u003cconsole\u003e:67\nnumCols: Int \u003d 38\nn: Long \u003d 766639\nsums: Array[Double] \u003d Array(1.2411869E8, 1.313453168E9, 2.780595069E9, 6.0, 0.0, 32.0, 43833.0, 78.0, 623371.0, 34568.0, 279.0, 162.0, 57729.0, 5268.0, 310.0, 4414.0, 0.0, 2.0, 3405.0, 6913326.0, 9303874.0, 1282.3899999999994, 1497.14, 39723.72000000001, 39953.109999999986, 756260.9699999996, 13292.690000000002, 113111.09999999995, 1.05413645E8, 1.57255352E8, 658529.119999999, 31197.48000000026, 88231.3499999989, 19878.400000000052, 1776.000000000001, 916.8499999999987, 41431.800000000105, 40048.72000000005)\nsumSquares: Array[Double] \u003d Array(1.3452455802400472E23, 9.284520970311343E31, 5.395641627069102E28, 36.0, 0.0, 39124.0, 1.1772419567E10, 1072.0, 3.536030557E9, 3.156349418109138E15, 2653.0, 2766.0, 3.145002086882085E15, 1.9135478E8, 3082.0, 811666.0, 0.0, 2.0, 222039.0, 6.589397919088578E15, 8.890420300115038E15, 8672.91353053, 6644.889076719998, 7.194289320501961E7, 6.959693378997868E7, 4.720408616442115E9, 3196219.257420431, 7.741286569367279E7, 4.6465907883593298E18, 1.3114568325665786E19, 3.5146533067086024E9, 2.4546624048921317E7, 9.416734826826265E7, 127424.75263790124, 44047.362955920784, 6052.879212650257, 6.808441269713098E7, 6.343286915490572E7)\nstdevs: Array[Double] \u003d Array(4.188951504920095E8, 1.1004854284147182E13, 2.6529318514282822E11, 0.006852602065360522, 0.0, 0.22590516646696115, 123.91884742877625, 0.037393862848970776, 67.90963885528959, 64164.83520144945, 0.05882539605827488, 0.060065894102794073, 64049.39246761144, 15.798802216301345, 0.06340332456453049, 1.0289314064177897, 0.0, 0.0016151727429825082, 0.5381512910778905, 92710.17712983645, 107687.59785070404, 0.10634898254620986, 0.09307924440243052, 9.687066531565373, 9.527810314981625, 78.46211700769851, 2.04177164574272, 10.047645421832357, 2461907.654089318, 4136009.6672808025, 67.70345626656726, 5.658342087376738, 11.082335640365384, 0.406865910885403, 0.23968685578859572, 0.08884770397884835, 9.423695657481783, 9.096086077247268)\nmeans: Array[Double] \u003d Array(161.89978594879727, 1713.2616107450833, 3626.994020653789, 7.826369386373508E-6, 0.0, 4.1740636727325376E-5, 0.05717554155215167, 1.017428020228556E-4, 0.8131219517921734, 0.04509032282469324, 3.6392617646636814E-4, 2.1131197343208472E-4, 0.07530141305099271, 0.00687155232123594, 4.043624182959646E-4, 0.005757599078575444, 0.0, 2.608789795457836E-6, 0.004441464626766966, 9.01770716073667, 12.13592577471274, 0.0016727429728985864, 0.0019528617771858725, 0.051815417686812186, 0.0521146328324022, 0.9864629506195218, 0.017338917013092214, 0.14754154171650535, 137.50102068900748, 205.123078789365, 0.858982024133913, 0.04069383373400031, 0.11508852275973294, 0.025929283535014593, 0.00231660533836656, 0.0011959344619827568, 0.05404342852372512, 0.05223934602857414)\nnormalize: (v: org.apache.spark.mllib.linalg.Vector)org.apache.spark.mllib.linalg.Vector\nnormalizedData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] \u003d MapPartitionsRDD[112] at map at \u003cconsole\u003e:84\nnormalizedDataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] \u003d ZippedPartitionsRDD2[114] at zip at \u003cconsole\u003e:86\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936327_-2040861998",
      "id": "20151224-130755_1820166210",
      "dateCreated": "2019-07-04 15:32:16.327",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// train the model\nimport org.apache.spark.mllib.clustering._\nval kmeans \u003d new KMeans()\nkmeans.setK(1) // find that one center\nkmeans.setRuns(10)\nval model \u003d kmeans.run(normalizedData)",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.328",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.mllib.clustering._\nkmeans: org.apache.spark.mllib.clustering.KMeans \u003d org.apache.spark.mllib.clustering.KMeans@5411f5\nres23: kmeans.type \u003d org.apache.spark.mllib.clustering.KMeans@5411f5\nres24: kmeans.type \u003d org.apache.spark.mllib.clustering.KMeans@5411f5\nmodel: org.apache.spark.mllib.clustering.KMeansModel \u003d org.apache.spark.mllib.clustering.KMeansModel@73d9bfa6\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936328_1262855984",
      "id": "20151224-130858_521933677",
      "dateCreated": "2019-07-04 15:32:16.328",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// see the centroid\nmodel.clusterCenters.foreach(centroid \u003d\u003e\n\tprintln(centroid.toString))\n\nval clusterAndLabel \u003d dataAndLabel.map { case\n\t(data, label) \u003d\u003e (model.predict(data), label)\n}\nval clusterLabelCount \u003d clusterAndLabel.countByValue \n\nclusterLabelCount.toList.sorted.foreach { case\n\t((cluster, label), count) \u003d\u003e\n\t\tprintln(f\"$cluster%1s$label%18s$count%8s\")\n}",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.329",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[2.670514605111177E-20,7.250679545857106E-25,3.2938248932737836E-23,-1.872691374892059E-16,0.0,-2.144796584038696E-17,-6.069566954993101E-18,3.0572358935278095E-16,-1.0119575002571815E-16,-2.2073267219843697E-21,-3.114513883650898E-16,-3.110899253206237E-17,-1.5569742772902257E-19,3.46587449943832E-17,-3.270499089762803E-16,8.45221085643208E-17,0.0,1.0712513419744075E-16,8.334596342732729E-16,1.1359846884107648E-18,1.7540950024421153E-19,3.0062603359748996E-16,-8.146820925274191E-16,5.280140934162351E-17,4.965111988485367E-16,1.8345408042053016E-18,-2.825992227901426E-16,2.11173198375324E-16,5.880653276505114E-19,-2.110163616456939E-19,1.006025799014661E-16,-1.871081010687547E-16,2.259653783488594E-16,-1.862767360664827E-15,6.357567269813054E-16,3.7052742542731693E-16,-2.699294796674466E-16,-4.461102748534956E-16]\nclusterAndLabel: org.apache.spark.rdd.RDD[(Int, String)] \u003d MapPartitionsRDD[43] at map at \u003cconsole\u003e:59\nclusterLabelCount: scala.collection.Map[(Int, String),Long] \u003d Map((0,normal.) -\u003e 766639)\n0           normal.  766639\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936328_476095051",
      "id": "20151224-130915_1509764392",
      "dateCreated": "2019-07-04 15:32:16.328",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// calculate distance between data point to centroid\ndef distToCentroid(datum: Vector, model: KMeansModel) \u003d {\n\tval centroid \u003d model.clusterCenters(model.predict(datum)) // if more than 1 center\n\tVectors.sqdist(datum, centroid)\n}",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.330",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "distToCentroid: (datum: org.apache.spark.mllib.linalg.Vector, model: org.apache.spark.mllib.clustering.KMeansModel)Double\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936329_986783506",
      "id": "20151224-130958_806366288",
      "dateCreated": "2019-07-04 15:32:16.329",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// decide threshold for anormalies\nval distances \u003d normalizedData.map(d \u003d\u003e distToCentroid(d, model)) // calculate the distance between datum and the centroid\nval threshold \u003d distances.top(3000).last // pick the last of the furthest 3000 points as the threshold",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.330",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "distances: org.apache.spark.rdd.RDD[Double] \u003d MapPartitionsRDD[98] at map at \u003cconsole\u003e:85\nthreshold: Double \u003d 10.507551778111415\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936330_-1005139264",
      "id": "20151224-131005_1663896157",
      "dateCreated": "2019-07-04 15:32:16.330",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//************************************\n//         Testing\n//************************************\n// load the data\nval rawTestdata \u003d sc.textFile(\"dataset/ad.all.csv\", 120)\nrawTestdata.count",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.331",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rawTestdata: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[86] at textFile at \u003cconsole\u003e:52\nres123: Long \u003d 4898431\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936331_-1195965310",
      "id": "20151224-131015_816765510",
      "dateCreated": "2019-07-04 15:32:16.331",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// parse input\nval testdataAndLabel \u003d rawTestdata.map { line \u003d\u003e\n\tval buffer \u003d ArrayBuffer[String]()\n\tbuffer.appendAll(line.split(\",\"))\n\tbuffer.remove(1, 3) // remove categorial attributes\n\tval label \u003d buffer.remove(buffer.length-1)\n\tval vector \u003d Vectors.dense(buffer.map(_.toDouble).toArray) \n\t(vector, label)\n}",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.332",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "testdataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] \u003d MapPartitionsRDD[87] at map at \u003cconsole\u003e:51\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936331_-1892539278",
      "id": "20151224-131210_658972449",
      "dateCreated": "2019-07-04 15:32:16.331",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// validation data\nval testdata \u003d testdataAndLabel.map(_._1).cache()",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.332",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "testdata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] \u003d MapPartitionsRDD[88] at map at \u003cconsole\u003e:53\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936332_1223322151",
      "id": "20151224-131221_95996253",
      "dateCreated": "2019-07-04 15:32:16.332",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// normalize validation data\nval normalizedTestData \u003d testdata.map(normalize(_))\nval normalizedTestDataAndLabel \u003d normalizedTestData.zip(testdataAndLabel.values) // put label back",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.333",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "normalizedTestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] \u003d MapPartitionsRDD[115] at map at \u003cconsole\u003e:89\nnormalizedTestDataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] \u003d ZippedPartitionsRDD2[117] at zip at \u003cconsole\u003e:90\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936333_-634749556",
      "id": "20151224-131231_847306018",
      "dateCreated": "2019-07-04 15:32:16.333",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// define distance measure\ndef distToCentroid(datum: Vector, model: KMeansModel) \u003d {\n\tval centroid \u003d model.clusterCenters(0) // the one center\n\tVectors.sqdist(datum, centroid)\n}",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.334",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "distToCentroid: (datum: org.apache.spark.mllib.linalg.Vector, model: org.apache.spark.mllib.clustering.KMeansModel)Double\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936333_1245996109",
      "id": "20151224-131244_870665071",
      "dateCreated": "2019-07-04 15:32:16.333",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// calculate distance of validation data points\nval testDistances \u003d normalizedTestData.map(d \u003d\u003e distToCentroid(d, model))",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.335",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "testDistances: org.apache.spark.rdd.RDD[Double] \u003d MapPartitionsRDD[118] at map at \u003cconsole\u003e:115\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936334_1651424277",
      "id": "20151224-131334_232414287",
      "dateCreated": "2019-07-04 15:32:16.334",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// get the anomalies\nval anomalies \u003d normalizedTestDataAndLabel.filter(\n\td \u003d\u003e distToCentroid(d._1, model) \u003e threshold  // threshold is calculated during training\n\t)\n\nval total \u003d anomalies.count()\nval trueLabel \u003d anomalies.filter(x \u003d\u003e x._2 !\u003d \"normal.\").count // count true \"anomalies\"",
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.335",
      "config": {
        "colWidth": 12.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "anomalies: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] \u003d MapPartitionsRDD[123] at filter at \u003cconsole\u003e:123\ntotal: Long \u003d 874423\ntrueLabel: Long \u003d 871043\n0.0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1562221936335_856144448",
      "id": "20151224-131455_1289150270",
      "dateCreated": "2019-07-04 15:32:16.335",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "jake",
      "dateUpdated": "2019-07-04 15:32:16.336",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1562221936336_382049291",
      "id": "20151224-131506_1394890797",
      "dateCreated": "2019-07-04 15:32:16.336",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "~Trash/Anomaly Detection",
  "id": "2EH57FAU2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}