{
  "paragraphs": [
    {
      "text": "%md\n# Overview\n## About raw data\n* 기상청 방재기상관측(AWS) 데이터 (API로 가져온 데이터 아님)\n* source : 기상자료 개방포털 - 기상관측 - 방재기상관측 - 자료 조회 [link](https://data.kma.go.kr/data/grnd/selectAwsRltmList.do?pgmNo\u003d56)\n* fname format (hdfs) : /raw/kma_aws/%d.csv\n* content format\n    \u003e 지점,일시,기온(°C),1분 강수량(mm),강수유무(유무),풍향(deg),풍속(m/s),현지기압(hPa),해면기압(hPa),습도(%),일사(MJ/m^2),일조(Sec)\n    610,2018-10-09 00:01,15.3,0,0,36.3,3,,,61.6,,\n    610,2018-10-09 00:02,15.3,0,0,37.7,2.6,,,61.9,,\n    610,2018-10-09 00:03,15.2,0,0,42.5,2.7,,,62,,\n\n\n## About formatted data\n* raw data -\u003e parquet dataframe으로 변환 (dh, temp, humid, wind, windVelo)\n* dh(datahour) : \"yyyy-mm-dd hh\"\n* temp : 온도,  humid : 습도\n* wind : 풍향, windVelo : 풍속\n\n* 홍성죽도 위치의 데이터를 받기 위함 그러나 홍성죽도에서는 강수량, 기압, 일사량 등의 데이터는 측정되지 않음\n* 따라서 상기 4개 칼럼만 생성\n",
      "user": "jake",
      "dateUpdated": "2019-07-15 16:43:57.125",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eOverview\u003c/h1\u003e\n\u003ch2\u003eAbout raw data\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003e기상청 방재기상관측(AWS) 데이터 (API로 가져온 데이터 아님)\u003c/li\u003e\n  \u003cli\u003esource : 기상자료 개방포털 - 기상관측 - 방재기상관측 - 자료 조회 \u003ca href\u003d\"https://data.kma.go.kr/data/grnd/selectAwsRltmList.do?pgmNo\u003d56\"\u003elink\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003efname format (hdfs) : /raw/kma_aws/%d.csv\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003econtent format\u003c/p\u003e\n  \u003cblockquote\u003e\n    \u003cp\u003e지점,일시,기온(°C),1분 강수량(mm),강수유무(유무),풍향(deg),풍속(m/s),현지기압(hPa),해면기압(hPa),습도(%),일사(MJ/m^2),일조(Sec)\u003cbr/\u003e610,2018-10-09 00:01,15.3,0,0,36.3,3,,,61.6,,\u003cbr/\u003e610,2018-10-09 00:02,15.3,0,0,37.7,2.6,,,61.9,,\u003cbr/\u003e610,2018-10-09 00:03,15.2,0,0,42.5,2.7,,,62,,\u003c/p\u003e\n  \u003c/blockquote\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAbout formatted data\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eraw data -\u0026gt; parquet dataframe으로 변환 (dh, temp, humid, wind, windVelo)\u003c/li\u003e\n  \u003cli\u003edh(datahour) : \u0026ldquo;yyyy-mm-dd hh\u0026rdquo;\u003c/li\u003e\n  \u003cli\u003etemp : 온도, humid : 습도\u003c/li\u003e\n  \u003cli\u003ewind : 풍향, windVelo : 풍속\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e홍성죽도 위치의 데이터를 받기 위함 그러나 홍성죽도에서는 강수량, 기압, 일사량 등의 데이터는 측정되지 않음\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e따라서 상기 4개 칼럼만 생성\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1540279567134_-335173325",
      "id": "20181010-102605_1830703070",
      "dateCreated": "2018-10-23 16:26:07.134",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import IntegerType\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nudf_floatize \u003d udf(lambda s: float(s), DoubleType())\nudf_dt2dh \u003d udf(lambda dhm: dhm[:13])\n\nsun \u003d spark.read.parquet(\u0027hdfs:///ds/sun.parquet\u0027).collect()\nawsdf_col \u003d [\u0027date_hm\u0027, \u0027temp\u0027, \u00271mRainfall\u0027, \u0027rainOrNot\u0027, \u0027wind\u0027, \u0027windVelo\u0027, \u0027humid\u0027]\n\ndef is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\ndef getsrs(day):\n    rs \u003d list(filter(lambda s: s[0] \u003d\u003d day, sun))\n    return {\u0027rise\u0027: rs[0][1],\n            \u0027set\u0027: rs[0][2]}\n\ndef reformatAWSDF(df):\n    oldcolnames \u003d df.schema.names\n    header \u003d df.first()\n    \n    # remove unnecessary row col\n    df \u003d df.filter(\u0027%s !\u003d \"%s\"\u0027 % (oldcolnames[0], header[0]))\\\n           .drop(oldcolnames[0], oldcolnames[7], oldcolnames[8], \n                 oldcolnames[10], oldcolnames[11])\n    del oldcolnames[0]\n    del oldcolnames[6]\n    del oldcolnames[6]\n    del oldcolnames[7]\n    del oldcolnames[7]\n    \n    # remove null\n    for colname in oldcolnames:\n        df \u003d df.filter(\u0027%s is not null\u0027 % colname)\n    \n    # df.show()\n\n    # change column name new\n    for idx in range(len(awsdf_col)):\n        if idx \u003d\u003d 0:\n            df \u003d df.withColumnRenamed(oldcolnames[idx], awsdf_col[idx])\n        else:\n            df \u003d df.withColumn(awsdf_col[idx], udf_floatize(oldcolnames[idx]))\n            \n    # select available columns\n    df \u003d df.select(\u0027date_hm\u0027, \u0027temp\u0027, \u00271mRainfall\u0027, \u0027rainOrNot\u0027,\n                   \u0027wind\u0027, \u0027windVelo\u0027, \u0027humid\u0027)\n    return df\n    \ndef getmerge_awsdf(filelist_hdfs):\n    # read dfs from hdfs\n    dfs \u003d []\n    for i in range(12):\n        dfs.append(spark.read.csv(\u0027hdfs:///raw/kma_aws/%d.csv\u0027 % (i+1)))\n        dfs[i] \u003d reformatAWSDF(dfs[i])\n\n    # merge dfs\n    merged_df \u003d dfs[0].union(dfs[1])\n    for i in range(9):\n        merged_df \u003d merged_df.union(dfs[i+2])\n\n    # make column of datehour\n    merged_df \u003d merged_df.withColumn(\u0027datehour\u0027, udf_dt2dh(df.date_hm))\\\n                         .drop(\u0027date_hm\u0027)\n       \n    # group by hour and avg\n    merged_df \u003d merged_df.groupBy(\u0027datehour\u0027)\\\n                         .avg(\u0027temp\u0027, \u0027humid\u0027, \u0027wind\u0027, \u0027windVelo\u0027)\\\n                         .sort(\u0027datehour\u0027)\n    \n    return merged_df\n",
      "user": "jake",
      "dateUpdated": "2018-10-23 16:26:07.135",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1540279567135_-1350147653",
      "id": "20181008-173818_238753838",
      "dateCreated": "2018-10-23 16:26:07.135",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "pyjake/file_기상청_방재기상관측(AWS) - e",
  "id": "2DVMYUY4M",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "sh:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}